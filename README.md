# Q-Learning Model

Проект реализующий Q-Learning модель.

Проект состоит из трех основных компонентов:

    Agent (DQNAgent.py) - моделька и её механизм обучения

    Game Environment (game.py) - среда игры в Змейку

    Training Script (main.py) - основной скрипт для обучения модели

## Архитектура нейронной сети

Модель использует простую полносвязную архитектуру:
python

class QModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)  # 11 → 256
        self.linear2 = nn.Linear(hidden_size, output_size) # 256 → 3

Модель получает 11-мерный вектор состояния:

    1 -> столкновение по текущему направлению
    2 -> столкновение при повороте направо
    3 -> столкновение при повороте налево
    4-7 -> текущее направление
    8-11 -> расположение еды относительно головы змейки

## Алгоритм обучения:

    Короткое (каждый шаг) и долгое обучение (размер батча 200, выбран методом проб и ошибок).
    Для обучения функции полезности используется уравнение Беллмана

## Система наград тщательно сбалансирована:

    +10 за сбор еды

    -10 за столкновение

    -20 за бездействие (если собирет много и будет бездействовать некоторое время, то ничего не снимется)

    +1 каждые 100 шагов без награды (поощрение выживания)

# Требования:

    pip install torch pygame numpy

# Запуск обучения

    python main.py

# Особенности игры
    
    В game.py нахдится сам скрипт змейки, в нем имеется параметр speed, который меняет скорости игры, 
    на данный момент там стоит 1000, для более быстрого обучения. Оптимально для глаза speed = 10.

    Сохрание модели: Автоматическое сохранение при улучшении рекорда

# Комментарии:
    
    В папке exit_model лежит обученная моделька с рекордом 93
    
